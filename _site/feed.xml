<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-26T17:30:16-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Blog</title><entry><title type="html">Pandas reference sheet - UGBA-88</title><link href="http://localhost:4000/2024/10/07/python-function-summary.html" rel="alternate" type="text/html" title="Pandas reference sheet - UGBA-88" /><published>2024-10-07T20:53:00-07:00</published><updated>2024-10-07T20:53:00-07:00</updated><id>http://localhost:4000/2024/10/07/python-function-summary</id><content type="html" xml:base="http://localhost:4000/2024/10/07/python-function-summary.html"><![CDATA[<p>We are using these alias in the reference sheet below</p>

<pre><code class="language-Python3">import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
</code></pre>

<h1 id="index">Index</h1>
<ul>
  <li><a href="#1">pd.read_csv(‘data-file.csv’)</a></li>
  <li><a href="#2">df.isnull().sum()</a></li>
  <li><a href="#3">df = df.dropna()</a></li>
  <li><a href="#4">plt.hist(df[‘column_name’])</a></li>
  <li><a href="#5">df.shape</a></li>
  <li><a href="#6">df.columns</a></li>
  <li><a href="#7">len(df)</a></li>
  <li><a href="#8">df.dtypes</a></li>
  <li><a href="#9">sns.barplot(data=df[‘Make’].value_counts(), orient=’h’)</a></li>
  <li><a href="#10">plt.figure(figsize=(15,10))</a></li>
  <li><a href="#11">plt.show()</a></li>
  <li><a href="#12">df[[‘X’, ‘Y’]].corr()</a></li>
  <li><a href="#13">plt.pie(category_counts, labels=category_counts.index, autopct=’%1.1f%%’)</a></li>
  <li><a href="#14">my_dataframe[“score”].std()</a></li>
  <li><a href="#15">np.std(my_dataframe[“score”], ddof=1)</a></li>
  <li><a href="#16">np.var(my_dataframe[“score”], ddof=1)</a></li>
  <li><a href="#17">my_dataframe[“score”].var(ddof=1)</a></li>
  <li><a href="#18">np.min(my_dataframe[“score”])</a></li>
  <li><a href="#19">np.ptp(my_dataframe[“score”])</a></li>
  <li><a href="#20">my_dataframe[“score”].mean()</a></li>
  <li><a href="#21">np.percentile(my_dataframe[“score”], 75)</a></li>
  <li><a href="#22">q3 = np.percentile(my_dataframe[“score”], 75)</a></li>
  <li><a href="#23">my_dataframe[“score”].quantile(0.75)</a></li>
  <li><a href="#24">my_dataframe[(my_dataframe[“score”] &gt; 50) &amp; (my_dataframe[“score”] &lt;= 100)]</a></li>
  <li><a href="#25">my_dataframe[“new_column”] = my_dataframe[“score”].apply(lambda x: x**2)</a></li>
  <li><a href="#26">import yfinance as yf</a></li>
  <li><a href="#27">pd.read_csv(“data.csv”).set_index(“Date”)</a></li>
  <li><a href="#28">my_dataframe.dropna()</a></li>
  <li><a href="#29">np.sqrt(my_dataframe[“score”])</a></li>
  <li><a href="#30">np.mean(my_dataframe[“score”])</a></li>
  <li><a href="#31">degrees of freedom</a></li>
  <li><a href="#32">model = sm.OLS(y, X).fit()</a></li>
</ul>

<h2 id="1">1</h2>
<pre><code class="language-Python3">pd.read_csv(‘data-file.csv’)
</code></pre>
<h1 id="read-a-csv-file-using-pandas-into-a-dataframe">Read a CSV file using pandas into a DataFrame</h1>
<p>Read a CSV file using pandas into a DataFrame| This method read_csv() reads the data-file.csv using pandas (pd) and returns a DataFrame object. A DataFrame object is similar to table with rows and columns but with lot more advanced data analysis and capabilities.It takes in the input the name of csv file in either single quotes or double quotes and returns output as DataFrame object (df)</p>

<h2 id="2">2</h2>
<pre><code class="language-Python3">df.isnull().sum()
</code></pre>
<h1 id="count-null-values-for-every-column-and-show-total-null-values-per-column">Count null values for every column and show total null values per column</h1>
<p>Count number of missing values. This method isnull() looks through all the rows and columns in your DataFrame object (df) where there is value missing (null, NaN or NAN) and returns True if there is a missing value and False if not missing .sum() will add up all the missing True values for each columns and rows in your DataFrame. Input is the df object and output is the column name and the count of all null values in that column. If you do df.isnull().sum().sum() you get the total count of all missing (null) values.</p>

<h2 id="3">3</h2>
<pre><code class="language-Python3">df = df.dropna()
</code></pre>
<h1 id="drop-rows-with-one-or-more-null-values">Drop rows with one or more null values</h1>
<p>By default, dropna() will drop any row that has at least one null value in any column.</p>

<h2 id="4">4</h2>
<pre><code class="language-Python3">plt.hist(df['column_name'], bins=np.arange(100,130,10))
</code></pre>
<h1 id="create-a-histogram-plot">Create a histogram plot</h1>
<p>The plt.hist() function in Matplotlib is used to create histograms, it takes in DataFrame an array-like object containing the values to be plotted, you can pass in a name of your column you want to plot the data in the ‘column_name’.
<code class="language-plaintext highlighter-rouge">plt.hist()</code>: This function creates a histogram.
<code class="language-plaintext highlighter-rouge">df['column_name']</code>: This is the data to be plotted from your dataframe df
<code class="language-plaintext highlighter-rouge">bins=np.arange(100,130,10)</code>: This argument specifies the bin edges for the histogram by generating an array [100,110,120]. It creates bins with a width of 10, starting from &gt;=100 and ending at &lt;130, note it does not include 130.</p>

<h2 id="5">5</h2>
<pre><code class="language-Python3">df.shape
</code></pre>
<h1 id="returns-the-dimensions-rows-columns-of-the-dataframe">Returns the dimensions (rows, columns) of the DataFrame</h1>
<p>df.shape returns the dimensions of a DataFrame as a tuple. It provides two values: (The number of rows, The number of columns)</p>

<h2 id="6">6</h2>
<pre><code class="language-Python3">df.columns
</code></pre>
<h1 id="shows-all-the-columns-you-have-in-your-dataframe">Shows all the columns you have in your DataFrame</h1>
<p>df.columns returns an Index object containing the column labels of a DataFrame. This allows you to see the names of all the columns in the DataFrame.</p>

<h2 id="7">7</h2>
<pre><code class="language-Python3">len(df)
</code></pre>
<h1 id="count-the-number-of-rows-you-have-in-your-dataframe">Count the number of rows you have in your DataFrame</h1>
<p>len(df) returns the number of rows in a DataFrame. It provides a quick way to determine how many records or observations are present.</p>

<h2 id="8">8</h2>
<pre><code class="language-Python3">df.dtypes
</code></pre>
<h1 id="display-data-type-of-each-column-in-dataframe">Display data type of each column in DataFrame</h1>
<p>df.dtypes returns a Series with the data types of each column in a DataFrame. This is useful for understanding the types of data you are working with, such as integers, floats, or objects (strings).</p>

<h2 id="9">9</h2>
<pre><code class="language-Python3">sns.barplot(data=df['Make'].value_counts(), orient='h')
</code></pre>
<h1 id="draws-a-horizontal-bar-plot-using-seaborn-library">Draws a horizontal bar plot using Seaborn library</h1>
<p><code class="language-plaintext highlighter-rouge">sns.barplot(data=df['Make'].value_counts(), orient='h')</code> uses the Seaborn library to create a horizontal bar plot. Here’s a breakdown of what it does: <code class="language-plaintext highlighter-rouge">df['Make'].value_counts()</code>: This counts the occurrences of each unique value in the ‘Make’ column of the DataFrame df.<code class="language-plaintext highlighter-rouge">sns.barplot(...)</code>: This creates a bar plot based on the counts, with the counts on the x-axis (since orient=’h’ specifies a horizontal orientation).</p>

<h2 id="10">10</h2>
<pre><code class="language-Python3">plt.figure(figsize=(15,10))
</code></pre>
<h1 id="draws-a-figure-with-15-inches-wide-and-10-inches-tall">Draws a figure with 15 inches wide and 10 inches tall</h1>
<p>The line <code class="language-plaintext highlighter-rouge">plt.figure(figsize=(15,10))</code> is used in Matplotlib to create a new figure for plotting with a specified size.
<code class="language-plaintext highlighter-rouge">plt</code>: Refers to the Matplotlib library alias, which is commonly imported as import matplotlib.pyplot as plt.
<code class="language-plaintext highlighter-rouge">figure(figsize=(15,10))</code>: Sets the figure size to 15 inches wide and 10 inches tall. This is useful for ensuring that your plots have the desired dimensions, making them clearer and more visually appealing.</p>

<h2 id="11">11</h2>
<pre><code class="language-Python3">plt.show()
</code></pre>
<h1 id="show-all-the-figures-plots-created">Show all the figures (plots) created</h1>
<p>The line <code class="language-plaintext highlighter-rouge">plt.show()</code> in Matplotlib is used to display all the figures that have been created. It renders the plot to the screen, allowing you to view the visualizations you have generated. You typically call <code class="language-plaintext highlighter-rouge">plt.show()</code> after defining your plots, including any titles, labels, or legends. Without this command, the plot may not be displayed, especially in non-interactive environments. Note you would also need to have run <code class="language-plaintext highlighter-rouge">%matplotlib inline</code> which is a magic command used in Jupyter notebooks to enable the inline display of Matplotlib plots. Make sure to run this command at the beginning of your notebook to activate inline plotting.</p>

<h2 id="12">12</h2>
<pre><code class="language-Python3">df[['X', 'Y']].corr()
</code></pre>
<h1 id="calculate-pearson-correlation-between-x-and-y-columns-of-data">Calculate Pearson correlation between X and Y columns of data</h1>
<p>code snippet <code class="language-plaintext highlighter-rouge">df[['X', 'Y']].corr()</code> calculates the correlation matrix between the columns X and Y in the DataFrame df. <code class="language-plaintext highlighter-rouge">df[['X', 'Y']]</code>: This selects the columns X and Y from the DataFrame. .corr(): This method computes the Pearson correlation coefficients, which measure the linear relationship between the selected columns. The result will be a 2x2 matrix showing the correlation values between X and Y, as well as each column’s correlation with itself (which is always 1).</p>

<h2 id="13">13</h2>
<pre><code class="language-Python3">plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%')
</code></pre>
<h1 id="draws-a-pie-chart-visualizing-proportions-of-each-category">Draws a pie chart visualizing proportions of each category</h1>
<p>The line plt.pie(category_counts, labels=category_counts.index, autopct=’%1.1f%%’) creates a pie chart using Matplotlib. Here’s a breakdown:</p>

<p><code class="language-plaintext highlighter-rouge">category_counts</code>: This should be a Series or array containing the data for the pie chart (e.g., counts of categories).
<code class="language-plaintext highlighter-rouge">labels=category_counts.index</code>: Sets the labels for each slice of the pie to the index of the category_counts.
<code class="language-plaintext highlighter-rouge">autopct='%1.1f%%'</code>: Formats the labels to show the percentage with one decimal place.
This generates a pie chart visualizing the proportions of each category. The <code class="language-plaintext highlighter-rouge">autopct='%1.1f%%'</code> parameter in the <code class="language-plaintext highlighter-rouge">plt.pie()</code> function specifies the format for displaying the percentage values on the pie chart slices. %1.1f: This part formats the percentage as a floating-point number with one digit before the decimal and one digit after the decimal point (e.g., 25.0).
%%: This represents the literal percent sign % in the output. So, each slice of the pie will show its percentage with one decimal place, like 25.0%.</p>

<h2 id="14">14</h2>
<pre><code class="language-Python3">my_dataframe["score"].std()
</code></pre>
<h1 id="calculate-the-standard-deviation-of-data-in-the-score-column-of-my_dataframe-variable-using-pandas">Calculate the standard deviation of data in the “score” column of my_dataframe variable using pandas</h1>
<p>The <code class="language-plaintext highlighter-rouge">std()</code> function calculates the standard deviation of the ‘score’ column in the my_dataframe DataFrame. The standard deviation is a measure of how spread out the data is from the average value. By default, Pandas assumes a sample standard deviation, i.e. ddof=1.</p>

<h2 id="15">15</h2>
<pre><code class="language-Python3">np.std(my_dataframe["score"], ddof=1)
</code></pre>
<h1 id="calculate-the-standard-deviation-of-data-in-the-score-column-of-my_dataframe-variable-using-numpy">Calculate the standard deviation of data in the “score” column of my_dataframe variable using numpy</h1>
<p>The <code class="language-plaintext highlighter-rouge">np.std()</code> function calculates the standard deviation of the ‘score’ column in the my_dataframe DataFrame. The standard deviation is a measure of how spread out the data is from the average value. By default, Numpy assumes a population standard deviation, so for us to calculate the sample standard deviation and get an unbiased estimate of the sample standard deviation, we can specify ddof=1 which provides an unbiased estimate of the population standard deviation from a sample. Specifying ddof=1 is also called as applying the Bessel’s Correction: This correction factor is applied to adjust for the bias that occurs when calculating the sample standard deviation from a sample rather than the entire population.</p>

<h2 id="16">16</h2>
<pre><code class="language-Python3">np.var(my_dataframe["score"], ddof=1)
</code></pre>
<h1 id="calculates-the-variance-using-numpy">Calculates the variance using NumPy</h1>
<p>Calculates the variance using NumPy for “Sleep_Hours” column in a DataFrame named my_dataframe. Variance is a statistical measure that quantifies the dispersion of a set of data points. A higher variance indicates that the data points are more spread out from the mean, while a lower variance indicates that the data points are more clustered around the mean. By default, Numpy assumes a population standard deviation, so for us to calculate the sample variance and get an unbiased estimate of the sample variance, we can specify ddof=1 which provides an unbiased estimate of the population variance from a sample. Specifying ddof=1 is also called as applying the Bessel’s Correction: This correction factor is applied to adjust for the bias that occurs when calculating the sample variance from a sample rather than the entire population.</p>

<h2 id="17">17</h2>
<pre><code class="language-Python3">my_dataframe["score"].var()
my_dataframe["score"].var(ddof=1)
</code></pre>
<h1 id="calculates-the-variance-using-pandas">Calculates the variance using Pandas</h1>
<p>The <code class="language-plaintext highlighter-rouge">var()</code> function calculates the variance of the ‘score’ column in the my_dataframe DataFrame. The variance is a measure of how spread out the data is from the average value. By default, Pandas assumes a sample variance, i.e. ddof=1.</p>

<h2 id="18">18</h2>
<pre><code class="language-Python3">np.min(my_dataframe["score"])
np.max(my_dataframe["score"])
len(my_dataframe["score"])
</code></pre>
<h1 id="calculates-minimum-maximum-and-count">Calculates Minimum, Maximum, and Count</h1>
<p><code class="language-plaintext highlighter-rouge">np.min</code>: Finds the smallest value in the “score” column.
<code class="language-plaintext highlighter-rouge">np.max</code>: Finds the largest value in the “score” column.
<code class="language-plaintext highlighter-rouge">len</code>: Counts the total number of data points in the “score” column.</p>

<h2 id="19">19</h2>
<pre><code class="language-Python3">np.ptp(my_dataframe["score"])
</code></pre>
<h1 id="range">Range</h1>
<p>The range is calculated using NumPy’s ptp() function, which computes the difference between the maximum and minimum values of the “score” column. This indicates the spread of data.</p>

<h2 id="20">20</h2>
<pre><code class="language-Python3">my_dataframe["score"].mean()
</code></pre>
<h1 id="mean">Mean</h1>
<p>The mean() function calculates the average of the “score” column in the DataFrame. It sums all values and divides by the number of data points, providing a measure of central tendency.</p>

<p>..</p>

<h2 id="21">21</h2>
<pre><code class="language-Python3">np.percentile(my_dataframe["score"], 75)
</code></pre>
<h1 id="percentile">Percentile</h1>
<p>The np.percentile function calculates the 75th percentile of the “score” column, identifying the value below which 75% of the data falls.</p>

<h2 id="22">22</h2>
<pre><code class="language-Python3">q3 = np.percentile(my_dataframe["score"], 75)
q1 = np.percentile(my_dataframe["score"], 25)
iqr = q3 - q1
</code></pre>
<h1 id="interquartile-range">Interquartile Range</h1>
<p>The interquartile range (IQR) measures the spread of the middle 50% of the data by subtracting the 25th percentile (Q1) from the 75th percentile (Q3).</p>

<h2 id="23">23</h2>
<pre><code class="language-Python3">my_dataframe["score"].quantile(0.75)
</code></pre>
<h1 id="quantile">Quantile</h1>
<p>The quantile function computes a specific quantile (e.g., 75th percentile) of the “score” column, equivalent to np.percentile.</p>

<h2 id="24">24</h2>
<pre><code class="language-Python3">my_dataframe[(my_dataframe["score"] &gt; 50) &amp; (my_dataframe["score"] &lt;= 100)]
</code></pre>
<h1 id="conditional-filtering">Conditional Filtering</h1>
<p>This expression filters rows where the “score” column is greater than 50 and less than or equal to 100. The &amp; operator combines multiple conditions.</p>

<h2 id="25">25</h2>
<pre><code class="language-Python3">my_dataframe["new_column"] = my_dataframe["score"].apply(lambda x: x**2)
</code></pre>
<h1 id="apply-function-to-create-a-new-column">Apply Function to Create a New Column</h1>
<p>The apply method applies a custom function (e.g., squaring values) to each element of the “score” column, generating a new column.</p>

<h2 id="26">26</h2>
<pre><code class="language-Python3">import yfinance as yf
import pandas as pd
import numpy as np
</code></pre>
<h1 id="this-code-imports-common-libraries-we-need-and-provides-a-shortcut-access-to-them">This code imports common libraries we need and provides a shortcut access to them</h1>
<p>yfinance: For financial data analysis.
pandas: For data manipulation.
numpy: For numerical computations.</p>

<h2 id="27">27</h2>
<pre><code class="language-Python3">pd.read_csv("data.csv").set_index("Date")
</code></pre>
<h1 id="read-csv-and-set-index">Read CSV and Set Index</h1>
<p>Reads a CSV file into a DataFrame and sets the “Date” column as the index, useful for time-series data.</p>

<h2 id="28">28</h2>
<pre><code class="language-Python3">my_dataframe.dropna()
</code></pre>
<h1 id="drop-missing-values">Drop Missing Values</h1>
<p>The dropna method removes rows with missing values from the DataFrame, ensuring clean data for analysis.</p>

<h2 id="29">29</h2>
<pre><code class="language-Python3">np.sqrt(my_dataframe["score"])
</code></pre>
<h1 id="square-root">Square Root</h1>
<p>The np.sqrt function computes the square root of each value in the “score” column.</p>

<h2 id="30">30</h2>
<pre><code class="language-Python3">np.mean(my_dataframe["score"])
np.std(my_dataframe["score"])
np.random.standard_t(df=10, size=100)
np.random.normal(loc=0, scale=1, size=100)
</code></pre>
<h1 id="mean-standard-deviation-and-random-numbers">Mean, Standard Deviation, and Random Numbers</h1>
<p>np.mean: Calculates the mean of the “score” column.
np.std: Computes the standard deviation of the “score” column.
np.random.standard_t: Generates random numbers from a Student’s t-distribution.
np.random.normal(loc=0, scale=1, size=100): Generates 100 random values from a standard normal (Z) distribution, with a mean (loc) of 0 and a standard deviation (scale) of 1. This distribution is commonly used for calculating Z-scores and standardizing data.</p>
<h2 id="31">31</h2>
<pre><code class="language-Python3">s1, s2 = 2.5, 3.0  # Sample standard deviations
n1, n2 = 30, 40    # Sample sizes

df = ((s1**2 / n1) + (s2**2 / n2))**2 / (
    ((s1**2 / n1)**2 / (n1 - 1)) + ((s2**2 / n2)**2 / (n2 - 1))
)
</code></pre>
<h1 id="degrees-of-freedom-for-two-sample-t-test-unequal-variances">Degrees of Freedom for Two-Sample t-Test (Unequal Variances)</h1>
<p>This formula calculates the degrees of freedom for a two-sample t-test when the variances of the two samples are unequal (Welch’s t-test). It accounts for the variances and sample sizes of the two groups and is a weighted approximation of the degrees of freedom. For comparing proportions between two groups, the degrees of freedom (df) typically aren’t calculated explicitly because the statistical test used, such as a Z-test for proportions, does not rely on a degrees of freedom calculation. Instead, the test relies on approximations of the normal distribution.</p>

<h2 id="32">32</h2>
<pre><code class="language-Python3">import pandas as pd
import statsmodels.api as sm

# Example data
data = {
    "independent_var1": [10, 20, 30, 40, 50],
    "independent_var2": [5, 10, 15, 20, 25],
    "dependent_var": [15, 25, 35, 45, 55]
}
df = pd.DataFrame(data)

# Define dependent and independent variables
X = df[["independent_var1", "independent_var2"]]  # Independent variables
y = df["dependent_var"]  # Dependent variable

# Add a constant (intercept term) to the independent variables
X = sm.add_constant(X)

# Fit the OLS model
model = sm.OLS(y, X).fit()

# Summary of the regression
print(model.summary())

</code></pre>
<h1 id="linear-regression">Linear Regression</h1>
<p>Adding a constant: The sm.add_constant(X) function ensures that an intercept term is included in the regression model. This is critical unless you explicitly know the intercept is zero.
Independent variables (X): These are the predictors you include in the model.
Dependent variable (y): The variable you’re trying to predict or explain.
Output:
The model.summary() provides a detailed summary, including coefficients, p-values, Rsquared and other key statistics.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[We are using these alias in the reference sheet below]]></summary></entry><entry><title type="html">DDOF - Delta Degrees of Freedom</title><link href="http://localhost:4000/2024/09/22/ddof.html" rel="alternate" type="text/html" title="DDOF - Delta Degrees of Freedom" /><published>2024-09-22T15:40:00-07:00</published><updated>2024-09-22T15:40:00-07:00</updated><id>http://localhost:4000/2024/09/22/ddof</id><content type="html" xml:base="http://localhost:4000/2024/09/22/ddof.html"><![CDATA[<p>Bessel’s correction - Delta Degrees of Freedom ddof=1 or using n-1 - why do we use n-1 ?</p>

<p>Numpy assume DDOF = 0 as default.
Pandas assume DDOF = 1 as default.</p>

<p>Stay tuned for the youtube video of this handwritten blog post which will give you detailed context about this topic with real life example ..</p>

<p><img src="/assets/images/DDOF-1.png" alt="Delta Degrees of Freedom" /></p>
<p>
<img src="/assets/images/DDOF-2.png" alt="Delta Degrees of Freedom" />
<p>
<img src="/assets/images/DDOF-3.png" alt="Delta Degrees of Freedom" />
<p>
<img src="/assets/images/DDOF-4.png" alt="Delta Degrees of Freedom" />
<p>
</p></p></p></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Bessel’s correction - Delta Degrees of Freedom ddof=1 or using n-1 - why do we use n-1 ?]]></summary></entry><entry><title type="html">EMBA22 Youtube videos by Kunal</title><link href="http://localhost:4000/mba/haas/2024/08/05/emba22-youtube-playlist.html" rel="alternate" type="text/html" title="EMBA22 Youtube videos by Kunal" /><published>2024-08-05T03:00:00-07:00</published><updated>2024-08-05T03:00:00-07:00</updated><id>http://localhost:4000/mba/haas/2024/08/05/emba22-youtube-playlist</id><content type="html" xml:base="http://localhost:4000/mba/haas/2024/08/05/emba22-youtube-playlist.html"><![CDATA[<h2 id="class-summaries-for-few-areas-below">Class summaries for few areas below</h2>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Link</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Macro</td>
      <td><a href="https://www.youtube.com/watch?v=ZkhJG-DscQY&amp;list=PL4KdJM8LzAMcRtgO4ONqbX0Fgl49oUgz2&amp;index=34">Youtube</a></td>
    </tr>
    <tr>
      <td>Strategy</td>
      <td><a href="https://www.youtube.com/watch?v=juLrfvXbGcQ&amp;list=PL4KdJM8LzAMcRtgO4ONqbX0Fgl49oUgz2&amp;index=58">Youtube</a></td>
    </tr>
    <tr>
      <td>Micro</td>
      <td><a href="https://www.youtube.com/watch?v=BAuO9E0tZvw&amp;list=PL4KdJM8LzAMcRtgO4ONqbX0Fgl49oUgz2&amp;index=66">Youtube</a></td>
    </tr>
    <tr>
      <td>Corporate Finance</td>
      <td><a href="https://www.youtube.com/watch?v=4IrEBsZSnj4&amp;list=PL4KdJM8LzAMcRtgO4ONqbX0Fgl49oUgz2&amp;index=72">Youtube</a></td>
    </tr>
    <tr>
      <td>Business Data Science</td>
      <td><a href="https://www.youtube.com/watch?v=02xpk1_4hmI&amp;list=PL4KdJM8LzAMcRtgO4ONqbX0Fgl49oUgz2&amp;index=96">Youtube</a></td>
    </tr>
    <tr>
      <td>Game Theory</td>
      <td><a href="https://www.youtube.com/watch?v=UtUX9acBlKI&amp;list=PL4KdJM8LzAMcRtgO4ONqbX0Fgl49oUgz2&amp;index=110">Youtube</a></td>
    </tr>
    <tr>
      <td>Leadership communication</td>
      <td><a href="https://www.youtube.com/watch?v=R_ErYygoRyY&amp;list=PL4KdJM8LzAMcRtgO4ONqbX0Fgl49oUgz2&amp;index=9">Youtube</a></td>
    </tr>
    <tr>
      <td>Entire playlist</td>
      <td><a href="https://www.youtube.com/playlist?list=PL4KdJM8LzAMcRtgO4ONqbX0Fgl49oUgz2">Youtube</a></td>
    </tr>
  </tbody>
</table>]]></content><author><name></name></author><category term="MBA" /><category term="HAAS" /><summary type="html"><![CDATA[Class summaries for few areas below]]></summary></entry><entry><title type="html">Designing Data Intesive Applications Video Summaries</title><link href="http://localhost:4000/software/2024/05/25/designing-data-intensive-applications-video-summaries.html" rel="alternate" type="text/html" title="Designing Data Intesive Applications Video Summaries" /><published>2024-05-25T11:34:57-07:00</published><updated>2024-05-25T11:34:57-07:00</updated><id>http://localhost:4000/software/2024/05/25/designing-data-intensive-applications-video-summaries</id><content type="html" xml:base="http://localhost:4000/software/2024/05/25/designing-data-intensive-applications-video-summaries.html"><![CDATA[<style>
   .youtube-video {
        aspect-ratio: 16 / 9;
        width: 100%;
    } 
</style>

<p>Check out the summaries of all the chapters from Designing Data Intensive Applications by <strong>Martin Kleppmann</strong>. I hope you like it and if you do please support Martin and buy his book, it has way more then what I summarize here!</p>

<p>Chapter 1 summary - Reliable Scalable Maintainable</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/PdtlXdse7pw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 2 Summary - Data Models</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/X-9R-DdDFrI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 3 Summary - Storage and Retrieval</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/CLMfk_n8exA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 4 Summary - Agile code evolution and data encoding</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/W5cWSNZY3l8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 5 Summary - Replication</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/8-dJt2t1vH4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 6 Summary - Partioning</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/YvRm6HVEJEE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 7 Summary - Transactions</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/MCx2j2lHxFk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 8 Summary - Troubles with Distributed Systems</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/oWKe5LL_f1E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 9 Summary - Consistency and Consensus</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/yIvft09RTAg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 10 Summary - Batch processing</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/zpDAqwQDrkY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 11 Summary - Stream processing</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/XiQNLGRAzss" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>Chapter 12 Summary - Future of Data Systems</p>
<iframe class="youtube-video" src="https://www.youtube.com/embed/wLj7A4A3OqQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=AW-16681818396"></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'AW-16681818396');
</script>]]></content><author><name></name></author><category term="software" /><summary type="html"><![CDATA[]]></summary></entry></feed>